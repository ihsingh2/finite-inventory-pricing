{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Inventory Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from algorithms import policy_iteration, q_learning, sarsa_lambda, value_iteration\n",
    "from constants import K, T, GAMMA, PRICES, NUM_EPISODES, TRACE_DECAY, GAUSSIAN_SMOOTHING_STD\n",
    "from model import sample_demand\n",
    "from scipy.ndimage import gaussian_filter, gaussian_filter1d\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(Q):\n",
    "    \"\"\" Use the state action value function to compute the value function. \"\"\"\n",
    "\n",
    "    V = {}\n",
    "    for inv in range(K + 1):\n",
    "        for t in range(T + 1):\n",
    "            V[(inv, t)] = 0.0\n",
    "\n",
    "    for (s, t, a) in Q.keys():\n",
    "        V[(s, t)] = max(V[(s, t)], Q[(s, t, a)])\n",
    "\n",
    "    return V\n",
    "\n",
    "def dict_to_array(dict):\n",
    "    \"\"\" Convert the value function from dictionary to numpy array. \"\"\"\n",
    "\n",
    "    array = np.zeros((K, T))\n",
    "    for inv in range(1, K + 1):\n",
    "        for t in range(1, T + 1):\n",
    "            array[inv - 1, t - 1] = dict[(inv, t)]\n",
    "    return array\n",
    "\n",
    "def state_value_function_to_array(dict):\n",
    "    \"\"\" Convert the value function from dictionary to numpy array. \"\"\"\n",
    "\n",
    "    price2idx = {price: idx for idx, price in enumerate(PRICES)}\n",
    "    array = np.zeros((K, T, len(PRICES)))\n",
    "    for inv in range(1, K + 1):\n",
    "        for t in range(1, T + 1):\n",
    "            for price in PRICES:\n",
    "                array[inv - 1, t - 1, price2idx[price]] = dict[(inv, t, price)]\n",
    "    return array\n",
    "\n",
    "def plot_matrix(data, title = '', fig = None, ax = None, sigma = 0.0, value_range=(PRICES[0], PRICES[-1])):\n",
    "    \"\"\" Plots the given matrix as a heatmap. \"\"\"\n",
    "\n",
    "    matrix = dict_to_array(data)\n",
    "    matrix = gaussian_filter(matrix, sigma=sigma)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        plt.imshow(matrix, cmap='viridis', aspect='auto')\n",
    "        cbar = plt.colorbar(label='Price')\n",
    "        plt.clim(*value_range)\n",
    "        plt.title(title)\n",
    "        plt.ylabel('Remaining Seats')\n",
    "        plt.xlabel('Days Left')\n",
    "        plt.ylim(1, K - 1)\n",
    "        plt.xlim(1, T - 1)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        im = ax.imshow(matrix, cmap='viridis', aspect='auto')\n",
    "        cbar = fig.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Price', fontsize=12)\n",
    "        im.set_clim(*value_range)\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.set_ylabel('Remaining Seats', fontsize=12)\n",
    "        ax.set_xlabel('Days Left', fontsize=12)\n",
    "        ax.set_ylim(1, K - 1)\n",
    "        ax.set_xlim(1, T - 1)\n",
    "\n",
    "def generate_trajectory(policy, num_runs=500):\n",
    "    \"\"\" Generates a trajectory following the given policy. \"\"\"\n",
    "\n",
    "    P = policy\n",
    "    trajectory = []\n",
    "    trajectory_reward = []\n",
    "\n",
    "    for run_idx in range(num_runs):\n",
    "\n",
    "        inventory = K\n",
    "        time = T\n",
    "        episode_reward = 0\n",
    "        trajectory.append([inventory])\n",
    "\n",
    "        while time > 0:\n",
    "\n",
    "            price = P[(inventory, time)]\n",
    "            demand = sample_demand(price)\n",
    "            sales = min(demand, inventory)\n",
    "            reward = price * sales\n",
    "            episode_reward = reward + GAMMA * episode_reward\n",
    "\n",
    "            inventory -= sales\n",
    "            time -= 1\n",
    "            trajectory[run_idx].append(inventory)\n",
    "\n",
    "        trajectory_reward.append(episode_reward)\n",
    "\n",
    "    return trajectory, trajectory_reward\n",
    "\n",
    "def plot_episode_rewards(metrics, title = 'Total Discounted Rewards per Episode', ax = None, label = None):\n",
    "    \"\"\" Plots the total discounted rewards per episode. \"\"\"\n",
    "\n",
    "    if label is None:\n",
    "        ax.plot(gaussian_filter1d(metrics['episode_rewards'], sigma=GAUSSIAN_SMOOTHING_STD))\n",
    "    else:\n",
    "        ax.plot(gaussian_filter1d(metrics['episode_rewards'], sigma=GAUSSIAN_SMOOTHING_STD), label=label)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel('Total Discounted Reward', fontsize=12)\n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_xlim(0, NUM_EPISODES)\n",
    "    ax.grid(linewidth=0.35)\n",
    "\n",
    "def plot_episode_regrets(metrics, title = 'Instantaneous Episodic Regret', ax = None, label = None):\n",
    "    \"\"\" Plots the instantaneous episodic regret. \"\"\"\n",
    "\n",
    "    if label is None:\n",
    "        ax.plot(gaussian_filter1d(metrics['episode_regrets'], sigma=GAUSSIAN_SMOOTHING_STD))\n",
    "    else:\n",
    "        ax.plot(gaussian_filter1d(metrics['episode_regrets'], sigma=GAUSSIAN_SMOOTHING_STD), label=label)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel('Regret', fontsize=12)\n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_xlim(0, NUM_EPISODES)\n",
    "    ax.grid(linewidth=0.35)\n",
    "\n",
    "def plot_cumulative_rewards(metrics, title = 'Cumulative Discounted Rewards over Episodes', ax = None, label = None):\n",
    "    \"\"\" Plots the cumulative discounted rewards over episode. \"\"\"\n",
    "\n",
    "    if label is None:\n",
    "        ax.plot(gaussian_filter1d(metrics['cumulative_rewards'], sigma=GAUSSIAN_SMOOTHING_STD))\n",
    "    else:\n",
    "        ax.plot(gaussian_filter1d(metrics['cumulative_rewards'], sigma=GAUSSIAN_SMOOTHING_STD), label=label)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel('Cumulative Discounted Reward', fontsize=12)\n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_xlim(0, NUM_EPISODES)\n",
    "    ax.grid(linewidth=0.35)\n",
    "\n",
    "def plot_cumulative_regrets(metrics, title = 'Cumulative Regrets over Episodes', ax = None, label = None):\n",
    "    \"\"\" Plots the cumulative regrets over episodes. \"\"\"\n",
    "\n",
    "    if label is None:\n",
    "        ax.plot(gaussian_filter1d(metrics['cumulative_regrets'], sigma=GAUSSIAN_SMOOTHING_STD))\n",
    "    else:\n",
    "        ax.plot(gaussian_filter1d(metrics['cumulative_regrets'], sigma=GAUSSIAN_SMOOTHING_STD), label=label)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel('Cumulative Regret', fontsize=12)\n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_xlim(0, NUM_EPISODES)\n",
    "    ax.grid(linewidth=0.35)\n",
    "\n",
    "def plot_state_value_evolution(metrics, title = '', ax = None):\n",
    "    \"\"\" Plots the evolution of state value function over episodes. \"\"\"\n",
    "\n",
    "    for idx in range(len(PRICES)):\n",
    "        ax.plot(gaussian_filter1d(np.array(metrics['starting_values'])[:, idx], sigma=GAUSSIAN_SMOOTHING_STD), label=f'p={PRICES[idx]}')\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel('Value Function', fontsize=12)\n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_xlim(0, NUM_EPISODES)\n",
    "    ax.grid(linewidth=0.35)\n",
    "\n",
    "def plot_policy_evolution(metrics, title = '', ax = None):\n",
    "    \"\"\" Plots the evolution of policy over episodes. \"\"\"\n",
    "\n",
    "    for idx in range(len(PRICES)):\n",
    "        ax.plot(metrics['starting_policies'])\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel('Price', fontsize=12)\n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_xlim(0, NUM_EPISODES)\n",
    "    ax.grid(linewidth=0.35)\n",
    "\n",
    "def save_plot(dest_path):\n",
    "    \"\"\" Saves the plot as an image file, given the absolute path. \"\"\"\n",
    "\n",
    "    plt.savefig(dest_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulation of Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_vi, policy_vi = value_iteration()\n",
    "trajectory_vi, reward_vi = generate_trajectory(policy_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_ql, policy_ql, metrics_ql = q_learning()\n",
    "trajectory_ql, reward_ql = generate_trajectory(policy_ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_sarsa, policy_sarsa, metrics_sarsa = sarsa_lambda()\n",
    "trajectory_sarsa, reward_sarsa = generate_trajectory(policy_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_idx in range(5):\n",
    "    if run_idx == 0:\n",
    "        plt.plot(trajectory_vi[run_idx], c='b', linewidth=0.5, label='Value Iteration')\n",
    "        plt.plot(trajectory_ql[run_idx], c='r', linewidth=0.5, label='Q Learning')\n",
    "        plt.plot(trajectory_sarsa[run_idx], c='g', linewidth=0.5, label='SARSA(λ)')\n",
    "    else:\n",
    "        plt.plot(trajectory_vi[run_idx], c='b', linewidth=0.5)\n",
    "        plt.plot(trajectory_ql[run_idx], c='r', linewidth=0.5)\n",
    "        plt.plot(trajectory_sarsa[run_idx], c='g', linewidth=0.5)\n",
    "plt.ylabel('Seats Left in Inventory')\n",
    "plt.xlabel('Number of Days Elapsed')\n",
    "plt.ylim(0, K)\n",
    "plt.xlim(0, T)\n",
    "plt.legend()\n",
    "plt.grid(linewidth=0.35)\n",
    "plt.savefig('../slides/figures/trajectory_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_idx in range(10):\n",
    "    if run_idx == 0:\n",
    "        plt.plot(trajectory_vi[run_idx], c='b', linewidth=0.5, label='Value Iteration')\n",
    "        plt.plot(trajectory_ql[run_idx], c='r', linewidth=0.5, label='Q Learning')\n",
    "        plt.plot(trajectory_sarsa[run_idx], c='g', linewidth=0.5, label='SARSA(λ)')\n",
    "    else:\n",
    "        plt.plot(trajectory_vi[run_idx], c='b', linewidth=0.5)\n",
    "        plt.plot(trajectory_ql[run_idx], c='r', linewidth=0.5)\n",
    "        plt.plot(trajectory_sarsa[run_idx], c='g', linewidth=0.5)\n",
    "plt.ylabel('Seats Left in Inventory')\n",
    "plt.xlabel('Number of Days Elapsed')\n",
    "plt.ylim(0, K)\n",
    "plt.xlim(0, T)\n",
    "plt.legend()\n",
    "plt.grid(linewidth=0.35)\n",
    "plt.savefig('../slides/figures/trajectory_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(np.array(trajectory_vi), axis=0), c='b', linewidth=1, label='Value Iteration')\n",
    "plt.plot(np.mean(np.array(trajectory_ql), axis=0), c='r', linewidth=1, label='Q Learning')\n",
    "plt.plot(np.mean(np.array(trajectory_sarsa), axis=0), c='g', linewidth=1, label='SARSA(λ)')\n",
    "plt.ylabel('Seats Left in Inventory')\n",
    "plt.xlabel('Number of Days Elapsed')\n",
    "plt.ylim(0, K)\n",
    "plt.xlim(0, T)\n",
    "plt.legend()\n",
    "plt.grid(linewidth=0.35)\n",
    "plt.savefig('../slides/figures/average_trajectory.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variation in Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_ql = value_function(Q_ql)\n",
    "V_sarsa = value_function(Q_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inventory in [K // 2, K]:\n",
    "    plt.plot(dict_to_array(V_vi)[inventory - 1], c='b', linewidth=0.75, label=f'Value Iteration')\n",
    "    plt.plot(dict_to_array(V_ql)[inventory - 1], c='r', linewidth=0.75, label=f'Q Learning')\n",
    "    plt.plot(dict_to_array(V_sarsa)[inventory - 1], c='g', linewidth=0.75, label=f'SARSA(λ)')\n",
    "    plt.ylabel('Value Function', fontsize=12)\n",
    "    plt.xlabel('Days Left', fontsize=12)\n",
    "    plt.ylim(0, 250000)\n",
    "    plt.xlim(0, T)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(linewidth=0.35)\n",
    "    plt.savefig(f'../slides/figures/inventory_{inventory}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestep in [T // 2, T]:\n",
    "    plt.plot(dict_to_array(V_vi)[:, timestep - 1], c='b', linewidth=0.75, label=f'Value Iteration')\n",
    "    plt.plot(dict_to_array(V_ql)[:, timestep - 1], c='r', linewidth=0.75, label=f'Q Learning')\n",
    "    plt.plot(dict_to_array(V_sarsa)[:, timestep - 1], c='g', linewidth=0.75, label=f'SARSA(λ)')\n",
    "    plt.ylabel('Value Function', fontsize=12)\n",
    "    plt.xlabel('Remaining Seats', fontsize=12)\n",
    "    plt.ylim(0, 250000)\n",
    "    plt.xlim(0, K)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(linewidth=0.35)\n",
    "    plt.savefig(f'../slides/figures/timestep_{timestep}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (inventory, time, color) in [(20, 20, 'b'), (20, 30, 'darkorange'), (30, 20, 'g'), (30, 30, 'r')]:\n",
    "    plt.plot(PRICES, state_value_function_to_array(Q_ql)[inventory - 1, time - 1], c=color, label=f'K={inventory}, T={time}')\n",
    "    plt.axhline(y=V_vi[inventory - 1, time - 1], c=color, linestyle=':')\n",
    "plt.ylabel('State Value Function', fontsize=12)\n",
    "plt.xlabel('Price', fontsize=12)\n",
    "plt.ylim(100000, 250000)\n",
    "plt.xlim(PRICES[0], PRICES[-1])\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(linewidth=0.35)\n",
    "plt.savefig(f'../slides/figures/state_action_ql.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (inventory, time, color) in [(20, 20, 'b'), (20, 30, 'darkorange'), (30, 20, 'g'), (30, 30, 'r')]:\n",
    "    plt.plot(PRICES, state_value_function_to_array(Q_sarsa)[inventory - 1, time - 1], c=color, label=f'K={inventory}, T={time}')\n",
    "    plt.axhline(y=V_vi[inventory - 1, time - 1], c=color, linestyle=':')\n",
    "plt.ylabel('State Value Function', fontsize=12)\n",
    "plt.xlabel('Price', fontsize=12)\n",
    "plt.ylim(100000, 250000)\n",
    "plt.xlim(PRICES[0], PRICES[-1])\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(linewidth=0.35)\n",
    "plt.savefig(f'../slides/figures/state_action_sarsa.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "plot_state_value_evolution(metrics_ql, ax=axs[0], title='Q-Learning')\n",
    "axs[0].legend(fontsize=12, loc='lower right')\n",
    "\n",
    "plot_state_value_evolution(metrics_sarsa, ax=axs[1], title='SARSA(λ)')\n",
    "axs[1].legend(fontsize=12, loc='lower right')\n",
    "\n",
    "save_plot('../slides/figures/value_evolution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "plot_policy_evolution(metrics_ql, ax=axs[0], title='Q-Learning')\n",
    "plot_policy_evolution(metrics_sarsa, ax=axs[1], title='SARSA(λ)')\n",
    "\n",
    "save_plot('../slides/figures/policy_evolution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learned Policies, Rewards and Regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "plot_matrix(V_vi, 'Value/Policy Iteration', fig, axs[0], 0.0, (0, 250000))\n",
    "plot_matrix(V_ql, 'Q-Learning', fig, axs[1], 0.0, (0, 250000))\n",
    "plot_matrix(V_sarsa, 'SARSA(λ)', fig, axs[2], 0.0, (0, 250000))\n",
    "\n",
    "save_plot('../slides/figures/value_function.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "plot_matrix(V_vi, 'Value/Policy Iteration', fig, axs[0], 1.0, (0, 250000))\n",
    "plot_matrix(V_ql, 'Q-Learning', fig, axs[1], 1.0, (0, 250000))\n",
    "plot_matrix(V_sarsa, 'SARSA(λ)', fig, axs[2], 1.0, (0, 250000))\n",
    "\n",
    "save_plot('../slides/figures/value_function_smooth.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "plot_matrix(policy_vi, 'Value/Policy Iteration', fig, axs[0])\n",
    "plot_matrix(policy_ql, 'Q-Learning', fig, axs[1])\n",
    "plot_matrix(policy_sarsa, 'SARSA(λ)', fig, axs[2])\n",
    "\n",
    "save_plot('../slides/figures/policy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "plot_matrix(policy_vi, 'Value/Policy Iteration', fig, axs[0], 1.0)\n",
    "plot_matrix(policy_ql, 'Q-Learning', fig, axs[1], 1.0)\n",
    "plot_matrix(policy_sarsa, 'SARSA(λ)', fig, axs[2], 1.0)\n",
    "\n",
    "save_plot('../slides/figures/policy_smooth.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "plot_episode_rewards(metrics_ql, ax=axs[0], label='Q-Learning')\n",
    "plot_episode_rewards(metrics_sarsa, ax=axs[0], label='SARSA(λ)')\n",
    "axs[0].legend(fontsize=12, loc='upper right')\n",
    "\n",
    "plot_cumulative_rewards(metrics_ql, ax=axs[1], label='Q-Learning')\n",
    "plot_cumulative_rewards(metrics_sarsa, ax=axs[1], label='SARSA(λ)')\n",
    "axs[1].legend(fontsize=12)\n",
    "\n",
    "save_plot('../slides/figures/reward.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "plot_episode_regrets(metrics_ql, ax=axs[0], label='Q-Learning')\n",
    "plot_episode_regrets(metrics_sarsa, ax=axs[0], label='SARSA(λ)')\n",
    "axs[0].legend(fontsize=12)\n",
    "\n",
    "plot_cumulative_regrets(metrics_ql, ax=axs[1], label='Q-Learning')\n",
    "plot_cumulative_regrets(metrics_sarsa, ax=axs[1], label='SARSA(λ)')\n",
    "axs[1].legend(fontsize=12)\n",
    "\n",
    "save_plot('../slides/figures/regret.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Effect of Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ability to modify hyperparameters on the fly has not been added for simplicity. Changes have to be made manually with kernel restarts for them to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, policy_100, metrics_100 = q_learning()\n",
    "with open('.tmp/epsilon_100.pkl', 'wb') as f:\n",
    "    pickle.dump([policy_100, metrics_100], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, policy_60, metrics_60 = q_learning()\n",
    "with open('.tmp/epsilon_60.pkl', 'wb') as f:\n",
    "    pickle.dump([policy_60, metrics_60], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, policy_20, metrics_20 = q_learning()\n",
    "with open('.tmp/epsilon_20.pkl', 'wb') as f:\n",
    "    pickle.dump([policy_20, metrics_20], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp/epsilon_100.pkl', 'rb') as f:\n",
    "    policy_100, metrics_100 = pickle.load(f)\n",
    "\n",
    "with open('.tmp/epsilon_60.pkl', 'rb') as f:\n",
    "    policy_60, metrics_60 = pickle.load(f)\n",
    "\n",
    "with open('.tmp/epsilon_20.pkl', 'rb') as f:\n",
    "    policy_20, metrics_20 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "plot_matrix(policy_20, 'ε=0.20', fig, axs[0], 1.0)\n",
    "plot_matrix(policy_60, 'ε=0.60', fig, axs[1], 1.0)\n",
    "plot_matrix(policy_100, 'ε=1.00', fig, axs[2], 1.0)\n",
    "\n",
    "save_plot('../slides/figures/epsilon_policy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "plot_episode_rewards(metrics_20, ax=axs[0], label='ε=0.20')\n",
    "plot_episode_rewards(metrics_60, ax=axs[0], label='ε=0.60')\n",
    "plot_episode_rewards(metrics_100, ax=axs[0], label='ε=1.00')\n",
    "axs[0].legend(fontsize=12, loc='upper left')\n",
    "\n",
    "plot_episode_regrets(metrics_20, ax=axs[1], label='ε=0.20')\n",
    "plot_episode_regrets(metrics_60, ax=axs[1], label='ε=0.60')\n",
    "plot_episode_regrets(metrics_100, ax=axs[1], label='ε=1.00')\n",
    "axs[1].legend(fontsize=12)\n",
    "\n",
    "save_plot('../slides/figures/epsilon_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Effect of Trace Decay Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ability to modify hyperparameters on the fly has not been added for simplicity. Changes have to be made manually with kernel restarts for them to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, policy_100, metrics_100 = sarsa_lambda()\n",
    "with open('.tmp/lambda_100.pkl', 'wb') as f:\n",
    "    pickle.dump([policy_100, metrics_100], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, policy_90, metrics_90 = sarsa_lambda()\n",
    "with open('.tmp/lambda_90.pkl', 'wb') as f:\n",
    "    pickle.dump([policy_90, metrics_90], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, policy_80, metrics_80 = sarsa_lambda()\n",
    "with open('.tmp/lambda_80.pkl', 'wb') as f:\n",
    "    pickle.dump([policy_80, metrics_80], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp/lambda_100.pkl', 'rb') as f:\n",
    "    policy_100, metrics_100 = pickle.load(f)\n",
    "\n",
    "with open('.tmp/lambda_90.pkl', 'rb') as f:\n",
    "    policy_90, metrics_90 = pickle.load(f)\n",
    "\n",
    "with open('.tmp/lambda_80.pkl', 'rb') as f:\n",
    "    policy_80, metrics_80 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "plot_matrix(policy_80, 'λ=0.80', fig, axs[0], 1.0)\n",
    "plot_matrix(policy_90, 'λ=0.90', fig, axs[1], 1.0)\n",
    "plot_matrix(policy_100, 'λ=1.00', fig, axs[2], 1.0)\n",
    "\n",
    "save_plot('../slides/figures/lambda_policy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "plot_episode_rewards(metrics_80, ax=axs[0], label='λ=0.80')\n",
    "plot_episode_rewards(metrics_90, ax=axs[0], label='λ=0.90')\n",
    "plot_episode_rewards(metrics_100, ax=axs[0], label='λ=1.00')\n",
    "axs[0].legend(fontsize=12, loc='upper left')\n",
    "\n",
    "plot_episode_regrets(metrics_80, ax=axs[1], label='λ=0.80')\n",
    "plot_episode_regrets(metrics_90, ax=axs[1], label='λ=0.90')\n",
    "plot_episode_regrets(metrics_100, ax=axs[1], label='λ=1.00')\n",
    "axs[1].legend(fontsize=12)\n",
    "\n",
    "save_plot('../slides/figures/lambda_metrics.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iiit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
